{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4151600e",
   "metadata": {},
   "source": [
    "# DeepFashion (TFDS) ‚Äì Image Classification with Keras/TensorFlow\n",
    "\n",
    "This notebook trains a multi-class image classifier for **clothing categories** using the **DeepFashion (In-Shop Clothes Retrieval)** dataset from **TensorFlow Datasets**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61aec7",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook presents the development of a **deep learning model for the automatic classification of clothing items from images**.  \n",
    "The main objective is to enable the model to correctly identify the **category of each garment** given a dataset of labeled fashion product images.\n",
    "\n",
    "This problem belongs to the field of **computer vision**, one of the most relevant areas in deep learning, with applications in **e-commerce** (such as the current challenge for the AI concentration), **visual product search**, and **personalized recommendation systems**.  \n",
    "\n",
    "To achieve this, a **convolutional neural network (CNN)** based on **MobileNetV3Small** was implemented using TensorFlow and Keras.  \n",
    "The network was trained and fine-tuned on the **Fashion Product Images (Small)** dataset from Kaggle, which contains thousands of labeled images of clothing and accessories.  \n",
    "The goal is to evaluate the model‚Äôs performance, optimize it through fine-tuning, and test its ability to generalize to unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3bfd6",
   "metadata": {},
   "source": [
    "### Dataset and Preprocessing\n",
    "\n",
    "The dataset used for this project is the **Fashion Product Images (Small)** dataset, available on Kaggle:  \n",
    "üëâ [Fashion Product Images (Small) ‚Äì Param Aggarwal](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small)\n",
    "\n",
    "This dataset contains approximately **4,400 images** of clothing items and accessories from the online fashion store **Myntra**, along with a metadata file (`styles.csv`) describing each product.\n",
    "\n",
    "#### Dataset description\n",
    "Each record in the dataset includes:\n",
    "- `id`: Unique identifier of the image file.\n",
    "- `gender`: Intended gender for the product.\n",
    "- `articleType`: Type or category of the clothing item (e.g., T-shirt, Shirt, Heels, Watch, etc.).\n",
    "- `baseColour`: Main color of the item.\n",
    "- `usage`: Suggested use or occasion (e.g., casual, formal).\n",
    "\n",
    "#### Download and organization\n",
    "The dataset was downloaded directly from Kaggle using the Kaggle API and extracted into a local directory.  \n",
    "Images were organized into a folder structure compatible with TensorFlow, divided into three subsets:\n",
    "\n",
    "data/\n",
    "raw/fashion_small/\n",
    "processed/fashion_kaggle_small\n",
    "train/\n",
    "val/\n",
    "test/\n",
    "\n",
    "#### Preprocessing steps\n",
    "1. All images were resized to **224√ó224 pixels**.  \n",
    "2. Pixel values were normalized to the range `[0, 1]`.  \n",
    "3. A **train/validation/test split** was applied (80% / 10% / 10%).  \n",
    "4. The top **10 most frequent categories** were selected to simplify the classification task and maintain balanced class distributions.  \n",
    "5. Basic **data augmentation** techniques were used to improve generalization, including random horizontal flips, rotations, and zoom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933a1d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.20.0 | Keras: 3.12.0\n"
     ]
    }
   ],
   "source": [
    "import os, math, json, shutil, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks\n",
    "\n",
    "# seeds for reproducibility\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
    "\n",
    "# Routes and constants\n",
    "RAW_DIR = \"data/raw/fashion_small\"                   \n",
    "CSV_PATH = os.path.join(RAW_DIR, \"styles.csv\")\n",
    "IMAGES_DIR = os.path.join(RAW_DIR, \"images\")\n",
    "\n",
    "# New output directory for processed data\n",
    "DATA_ROOT = \"data/processed/fashion_kaggle_small\"\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10      # global: ~80/10/10\n",
    "BATCH = 16\n",
    "\n",
    "def _safe_save_img(src_path, dst_path, size=IMG_SIZE):\n",
    "    \"\"\"Opens an image from src_path, resizes it to size and saves it as JPEG to dst_path.\"\"\"\n",
    "    try:\n",
    "        with Image.open(src_path) as im:\n",
    "            im = im.convert(\"RGB\").resize(size)\n",
    "            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "            im.save(dst_path, format=\"JPEG\", quality=95)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7ac87",
   "metadata": {},
   "source": [
    "### Data loading, class filtering, and stratified split\n",
    "\n",
    "- **Load metadata:** We read `styles.csv` and keep only rows with non-null `id` and `articleType`.  \n",
    "  Both fields are cast to `str` to ensure consistent joins with image filenames.\n",
    "\n",
    "- **Category frequency & selection:** We compute the frequency of `articleType` and keep the **Top-10 most frequent categories** (`TOP_K=10`).  \n",
    "  This reduces noise from very small classes and yields a more balanced multi-class problem while keeping the task realistic.\n",
    "\n",
    "- **Shuffle:** We randomly shuffle the filtered dataframe with a fixed seed (`SEED=42`) for full reproducibility.\n",
    "\n",
    "- **Split strategy (stratified):**\n",
    "  1. First, a **test split** of `TEST_FRACTION = 0.10` is created with stratification by `articleType` to preserve class proportions.\n",
    "  2. From the remaining 90%, we carve out a **validation split** such that the final proportions are **80/10/10** (train/val/test).  \n",
    "- **Outputs:**  \n",
    "  - `train_df`, `val_df`, `test_df` with their lengths printed for verification.  \n",
    "  - Class counts shown before and after filtering to document the selected categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf247b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes (articleType): 143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "articleType\n",
       "Tshirts                  7067\n",
       "Shirts                   3217\n",
       "Casual Shoes             2845\n",
       "Watches                  2542\n",
       "Sports Shoes             2036\n",
       "Kurtas                   1844\n",
       "Tops                     1762\n",
       "Handbags                 1759\n",
       "Heels                    1323\n",
       "Sunglasses               1073\n",
       "Wallets                   936\n",
       "Flip Flops                914\n",
       "Sandals                   897\n",
       "Briefs                    849\n",
       "Belts                     813\n",
       "Backpacks                 724\n",
       "Socks                     686\n",
       "Formal Shoes              637\n",
       "Perfume and Body Mist     613\n",
       "Jeans                     609\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classes: ['Casual Shoes', 'Handbags', 'Heels', 'Kurtas', 'Shirts', 'Sports Shoes', 'Sunglasses', 'Tops', 'Tshirts', 'Watches']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "articleType\n",
       "Tshirts         7067\n",
       "Shirts          3217\n",
       "Casual Shoes    2845\n",
       "Watches         2542\n",
       "Sports Shoes    2036\n",
       "Kurtas          1844\n",
       "Tops            1762\n",
       "Handbags        1759\n",
       "Heels           1323\n",
       "Sunglasses      1073\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 25468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20374, 2547, 2547)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv \n",
    "df = pd.read_csv(CSV_PATH, on_bad_lines=\"skip\")\n",
    "df = df.dropna(subset=[\"id\", \"articleType\"]).copy()\n",
    "df[\"id\"] = df[\"id\"].astype(str)\n",
    "df[\"articleType\"] = df[\"articleType\"].astype(str)\n",
    "\n",
    "# Count by category\n",
    "counts = df[\"articleType\"].value_counts()\n",
    "print(\"Total classes (articleType):\", counts.shape[0])\n",
    "display(counts.head(20))\n",
    "\n",
    "# Filter classes \n",
    "TOP_K = 10\n",
    "keep_classes = set(counts.head(TOP_K).index)\n",
    "\n",
    "df = df[df[\"articleType\"].isin(keep_classes)].sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Final Classes:\", sorted(keep_classes))\n",
    "display(df[\"articleType\"].value_counts())\n",
    "print(\"Total examples:\", len(df))\n",
    "\n",
    "# Split stratified\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=TEST_FRACTION, stratify=df[\"articleType\"], random_state=SEED\n",
    ")\n",
    "\n",
    "# Of what remains, allocate validation set\n",
    "val_size = VAL_FRACTION / (1 - TEST_FRACTION)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=val_size, stratify=train_val_df[\"articleType\"], random_state=SEED\n",
    ")\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d3c06",
   "metadata": {},
   "source": [
    "### Image export and directory structure\n",
    "\n",
    "To make the dataset compatible with TensorFlow‚Äôs `image_dataset_from_directory()` API,  \n",
    "the images are reorganized into a structured folder hierarchy based on their split and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06511c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: guardadas=20372, fallidas=2\n",
      "val: guardadas=2546, fallidas=1\n",
      "test: guardadas=2547, fallidas=0\n"
     ]
    }
   ],
   "source": [
    "# Clean output directories\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_dir = os.path.join(DATA_ROOT, split)\n",
    "    if os.path.exists(split_dir):\n",
    "        shutil.rmtree(split_dir)\n",
    "\n",
    "def _export_split(split_df, split_name):\n",
    "    ok, bad = 0, 0\n",
    "    for _, row in split_df.iterrows():\n",
    "        img_id = row[\"id\"]\n",
    "        cls = row[\"articleType\"]\n",
    "        src = os.path.join(IMAGES_DIR, f\"{img_id}.jpg\")\n",
    "        dst = os.path.join(DATA_ROOT, split_name, cls, f\"{cls}_{img_id}.jpg\")\n",
    "        if _safe_save_img(src, dst):\n",
    "            ok += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "    print(f\"{split_name}: guardadas={ok}, fallidas={bad}\")\n",
    "\n",
    "_export_split(train_df, \"train\")\n",
    "_export_split(val_df, \"val\")\n",
    "_export_split(test_df, \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aefa3a",
   "metadata": {},
   "source": [
    "### Class distribution analysis\n",
    "\n",
    "Before training, it is important to verify that the dataset remains **balanced** after preprocessing and exporting.\n",
    "\n",
    "The function `class_distribution()`:\n",
    "- Iterates through the class folders under the specified directory (`train`, in this case).\n",
    "- Counts the number of image files (`.jpg`, `.jpeg`, `.png`) in each class subdirectory.\n",
    "- Returns a Pandas `Series` sorted in descending order for inspection.\n",
    "- Plots a **bar chart** using Matplotlib to visualize the class balance.\n",
    "\n",
    "This step helps confirm that the top 10 selected categories remain reasonably balanced and that no class was disproportionately affected during preprocessing or export.\n",
    "\n",
    "The plot titled **‚ÄúClass distribution (train)‚Äù** provides a quick overview of how many training samples exist for each clothing type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7ed76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tshirts         5652\n",
       "Shirts          2572\n",
       "Casual Shoes    2275\n",
       "Watches         2034\n",
       "Sports Shoes    1628\n",
       "Kurtas          1476\n",
       "Tops            1410\n",
       "Handbags        1407\n",
       "Heels           1059\n",
       "Sunglasses       859\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def class_distribution(root_dir):\n",
    "    counts = {}\n",
    "    for c in sorted(os.listdir(root_dir)):\n",
    "        cdir = os.path.join(root_dir, c)\n",
    "        if os.path.isdir(cdir):\n",
    "            n = len([f for f in os.listdir(cdir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
    "            counts[c] = n\n",
    "    s = pd.Series(counts).sort_values(ascending=False)\n",
    "    display(s)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,4))\n",
    "    s.plot(kind=\"bar\")\n",
    "    plt.title(\"Class distribution (train)\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Images\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return s\n",
    "\n",
    "dist_train = class_distribution(os.path.join(DATA_ROOT, \"train\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a8708",
   "metadata": {},
   "source": [
    "### Dataset loading and augmentation pipeline\n",
    "\n",
    "After exporting the images into a class-based directory structure, the next step is to create TensorFlow datasets for efficient training.\n",
    "\n",
    "**Function: `load_ds()`**\n",
    "- **Purpose:** Loads the train and validation datasets from the processed directory and applies real-time image augmentation.\n",
    "- **Implementation details:**\n",
    "  - Uses `keras.utils.image_dataset_from_directory()` to automatically label subfolders by class name.\n",
    "  - Images are resized to **224√ó224** and batched (`batch_size=16`).\n",
    "  - Labels are returned in **categorical (one-hot)** format.\n",
    "  - `seed` ensures deterministic shuffling and split reproducibility.\n",
    "\n",
    "**Data augmentation:**  \n",
    "A lightweight augmentation pipeline is applied only to the training dataset using a `keras.Sequential` model that includes:\n",
    "- `RandomFlip(\"horizontal\")`  \n",
    "- `RandomRotation(0.05)`  \n",
    "- `RandomZoom(0.1)`\n",
    "\n",
    "These transformations improve generalization by introducing variability without altering semantic meaning.\n",
    "\n",
    "**Performance optimization:**  \n",
    "Both datasets are cached and prefetched with `AUTOTUNE` to fully utilize available CPU/GPU resources and improve training throughput.\n",
    "\n",
    "**Output:**  \n",
    "- `ds_train`: augmented training dataset  \n",
    "- `ds_val`: validation dataset (not augmented, deterministic order)  \n",
    "- `class_names`: list of the 10 target clothing categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f7dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20372 files belonging to 10 classes.\n",
      "Found 2546 files belonging to 10 classes.\n",
      "Clases: ['Casual Shoes', 'Handbags', 'Heels', 'Kurtas', 'Shirts', 'Sports Shoes', 'Sunglasses', 'Tops', 'Tshirts', 'Watches']\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def load_ds(root, img_size=(224,224), batch_size=16, seed=42):\n",
    "    ds_train = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"train\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        seed=seed\n",
    "    )\n",
    "    class_names_ds = ds_train.class_names\n",
    "    ds_val = keras.utils.image_dataset_from_directory(\n",
    "        os.path.join(root, \"val\"),\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode=\"categorical\",\n",
    "        seed=seed,\n",
    "        shuffle=False\n",
    "    )\n",
    "    aug = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ])\n",
    "    ds_train = ds_train.map(lambda x,y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds_train.prefetch(AUTOTUNE), ds_val.prefetch(AUTOTUNE), class_names_ds\n",
    "\n",
    "ds_train, ds_val, class_names = load_ds(DATA_ROOT, batch_size=BATCH)\n",
    "print(\"Clases:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2acc0",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "We adopt **transfer learning** with a lightweight convolutional backbone: **MobileNetV3Small** pre-trained on **ImageNet**.  \n",
    "The classification head is customized for our **10-class** fashion task.\n",
    "\n",
    "**Architecture**\n",
    "- **Backbone:** `keras.applications.MobileNetV3Small(include_top=False, weights=\"imagenet\")`\n",
    "- **Preprocessing:** `keras.applications.mobilenet_v3.preprocess_input` applied to RGB inputs of shape **224√ó224√ó3**\n",
    "- **Head:** `GlobalAveragePooling2D` ‚Üí `Dropout(p=0.3)` ‚Üí `Dense(NUM_CLASSES, activation=\"softmax\")`\n",
    "- **Regularization:** L2 weight decay (`l2_reg=1e-5`) on the final Dense layer to reduce overfitting\n",
    "\n",
    "**Training setup**\n",
    "- **Backbone frozen:** `train_backbone=False` (we only train the classification head)\n",
    "- **Loss:** `categorical_crossentropy` (labels are one-hot encoded via `label_mode=\"categorical\"`)\n",
    "- **Optimizer:** `Adam(lr=1e-3)`\n",
    "- **Batch size:** 16\n",
    "- **Epochs:** 10\n",
    "\n",
    "**Outputs**\n",
    "- Trained baseline model saved to **`runs/best.keras`**\n",
    "- Class name mapping saved to **`runs/class_names.json`**\n",
    "- Training history stored in **`runs/train_log.csv`**\n",
    "- `model.summary()` printed for full layer configuration and parameter counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27761249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 71ms/step - accuracy: 0.8060 - loss: 0.5364 - val_accuracy: 0.8845 - val_loss: 0.2901 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 58ms/step - accuracy: 0.8708 - loss: 0.3379 - val_accuracy: 0.8955 - val_loss: 0.2632 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 58ms/step - accuracy: 0.8839 - loss: 0.3075 - val_accuracy: 0.9042 - val_loss: 0.2461 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.8862 - loss: 0.2963 - val_accuracy: 0.9049 - val_loss: 0.2411 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 60ms/step - accuracy: 0.8905 - loss: 0.2904 - val_accuracy: 0.9093 - val_loss: 0.2391 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 59ms/step - accuracy: 0.8871 - loss: 0.2929 - val_accuracy: 0.9108 - val_loss: 0.2375 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.8909 - loss: 0.2863 - val_accuracy: 0.9108 - val_loss: 0.2322 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 65ms/step - accuracy: 0.8939 - loss: 0.2815 - val_accuracy: 0.9069 - val_loss: 0.2310 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 241ms/step - accuracy: 0.8910 - loss: 0.2847 - val_accuracy: 0.9077 - val_loss: 0.2352 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.8913 - loss: 0.2817 - val_accuracy: 0.9124 - val_loss: 0.2248 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling2d        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,770</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      ‚îÇ       \u001b[38;5;34m939,120\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_average_pooling2d        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             ‚îÇ         \u001b[38;5;34m5,770\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">956,432</span> (3.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m956,432\u001b[0m (3.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,770</span> (22.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,770\u001b[0m (22.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,542</span> (45.09 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m11,542\u001b[0m (45.09 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(num_classes, input_shape=(224,224,3), dropout=0.3, l2_reg=1e-5, train_backbone=False):\n",
    "    base = keras.applications.MobileNetV3Small(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "    base.trainable = train_backbone\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(inputs)\n",
    "    x = base(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "NUM_CLASSES = len(class_names)\n",
    "EPOCHS_BASE = 10     \n",
    "LR_BASE = 1e-3\n",
    "\n",
    "model = build_model(num_classes=NUM_CLASSES, train_backbone=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(LR_BASE),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_base = model.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_BASE, callbacks=cbs)\n",
    "\n",
    "# Keep detected classes\n",
    "with open(\"runs/class_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(class_names, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026dc18",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "After training the baseline model, we fine-tuned it to improve performance.\n",
    "\n",
    "**Process**\n",
    "- Reload the best baseline model (`runs/best.keras`).\n",
    "- Unfreeze the last **30%** of the MobileNetV3Small layers so they can adjust to our dataset.\n",
    "- Keep the earlier layers frozen to retain general ImageNet features.\n",
    "\n",
    "**Training setup**\n",
    "- Learning rate: `1e-5`\n",
    "- Epochs: 8\n",
    "- Optimizer: Adam\n",
    "- Loss: Categorical Crossentropy\n",
    "- Metric: Accuracy\n",
    "\n",
    "**Goal**\n",
    "Fine-tuning helps the network learn more clothing-specific patterns and textures while keeping useful pre-trained weights.  \n",
    "This step slightly increased validation accuracy and improved overall generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01db9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 75ms/step - accuracy: 0.8136 - loss: 0.5844 - val_accuracy: 0.9152 - val_loss: 0.2332 - learning_rate: 1.0000e-05\n",
      "Epoch 2/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 73ms/step - accuracy: 0.8567 - loss: 0.3920 - val_accuracy: 0.9136 - val_loss: 0.2266 - learning_rate: 1.0000e-05\n",
      "Epoch 3/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 73ms/step - accuracy: 0.8802 - loss: 0.3313 - val_accuracy: 0.9156 - val_loss: 0.2170 - learning_rate: 1.0000e-05\n",
      "Epoch 4/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 74ms/step - accuracy: 0.8852 - loss: 0.3087 - val_accuracy: 0.9203 - val_loss: 0.2110 - learning_rate: 1.0000e-05\n",
      "Epoch 5/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 74ms/step - accuracy: 0.8934 - loss: 0.2843 - val_accuracy: 0.9203 - val_loss: 0.2013 - learning_rate: 1.0000e-05\n",
      "Epoch 6/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 74ms/step - accuracy: 0.9005 - loss: 0.2673 - val_accuracy: 0.9254 - val_loss: 0.1949 - learning_rate: 1.0000e-05\n",
      "Epoch 7/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 74ms/step - accuracy: 0.9013 - loss: 0.2615 - val_accuracy: 0.9246 - val_loss: 0.1911 - learning_rate: 1.0000e-05\n",
      "Epoch 8/8\n",
      "\u001b[1m1274/1274\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 84ms/step - accuracy: 0.9076 - loss: 0.2443 - val_accuracy: 0.9254 - val_loss: 0.1858 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "UNFREEZE_RATIO = 0.30\n",
    "EPOCHS_FT = 8\n",
    "LR_FT = 1e-5\n",
    "\n",
    "model_ft = keras.models.load_model(\"runs/best.keras\")\n",
    "\n",
    "# Find backbone\n",
    "backbone = None\n",
    "for lyr in model_ft.layers:\n",
    "    if isinstance(lyr, keras.Model) and len(lyr.layers) > 10:\n",
    "        backbone = lyr; break\n",
    "assert backbone is not None, \"Backbone no encontrado\"\n",
    "\n",
    "n = len(backbone.layers)\n",
    "cut = int(n * (1 - UNFREEZE_RATIO))\n",
    "for i, layer in enumerate(backbone.layers):\n",
    "    layer.trainable = (i >= cut)\n",
    "\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(LR_FT),\n",
    "                 loss=\"categorical_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "cbs_ft = [\n",
    "    callbacks.ModelCheckpoint(\"runs/best_finetune.keras\", monitor=\"val_accuracy\", save_best_only=True),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3),\n",
    "    callbacks.CSVLogger(\"runs/train_log_finetune.csv\", append=False),\n",
    "]\n",
    "\n",
    "hist_ft = model_ft.fit(ds_train, validation_data=ds_val, epochs=EPOCHS_FT, callbacks=cbs_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83c97c",
   "metadata": {},
   "source": [
    "### Results and Evaluation (Test Set)\n",
    "\n",
    "We evaluate the final model on the **held-out test set**:\n",
    "\n",
    "**Procedure**\n",
    "- Load the best available model:\n",
    "  - `runs/best_finetune.keras` if present, otherwise `runs/best.keras`.\n",
    "- Build a fixed (non-shuffled) test tensor by iterating over the directory with\n",
    "  `image_dataset_from_directory(..., shuffle=False)`.\n",
    "- Compute predictions (`softmax` scores), then take `argmax` for class labels.\n",
    "\n",
    "**Metrics reported**\n",
    "- **Accuracy** (`accuracy_score`): overall proportion of correctly classified images.\n",
    "- **F1-macro** (`f1_score(..., average=\"macro\")`): F1 averaged across classes (treats all classes equally).\n",
    "- **Classification report** (`precision`, `recall`, `f1-score`, `support`) per class.\n",
    "\n",
    "**Confusion Matrix**\n",
    "- A confusion matrix is generated to visualize per-class performance and common confusions.\n",
    "- The figure is saved to **`reports/figures/confusion_matrix.png`** for inclusion in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d561769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2547 files belonging to 10 classes.\n",
      "Test accuracy = 0.922654102866117\n",
      "Test F1 macro = 0.9198700985744075\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Casual Shoes     0.8547    0.8877    0.8709       285\n",
      "    Handbags     0.9767    0.9545    0.9655       176\n",
      "       Heels     0.9603    0.9167    0.9380       132\n",
      "      Kurtas     0.9617    0.9565    0.9591       184\n",
      "      Shirts     0.9474    0.9503    0.9488       322\n",
      "Sports Shoes     0.8586    0.8333    0.8458       204\n",
      "  Sunglasses     1.0000    1.0000    1.0000       107\n",
      "        Tops     0.7238    0.7443    0.7339       176\n",
      "     Tshirts     0.9406    0.9406    0.9406       707\n",
      "     Watches     0.9961    0.9961    0.9961       254\n",
      "\n",
      "    accuracy                         0.9227      2547\n",
      "   macro avg     0.9220    0.9180    0.9199      2547\n",
      "weighted avg     0.9234    0.9227    0.9229      2547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def load_all(split_dir):\n",
    "    ds = keras.utils.image_dataset_from_directory(\n",
    "        split_dir, image_size=IMG_SIZE, batch_size=32, label_mode=\"categorical\", shuffle=False\n",
    "    )\n",
    "    Xs, Ys = [], []\n",
    "    for x,y in ds:\n",
    "        Xs.append(x.numpy()); Ys.append(y.numpy())\n",
    "    return np.vstack(Xs), np.vstack(Ys), ds.class_names\n",
    "\n",
    "X_test, Y_test, class_names_test = load_all(os.path.join(DATA_ROOT, \"test\"))\n",
    "m_final_path = \"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\"\n",
    "m_final = keras.models.load_model(m_final_path)\n",
    "\n",
    "P = m_final.predict(X_test, verbose=0)\n",
    "y_true = Y_test.argmax(1); y_pred = P.argmax(1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(\"Test accuracy =\", acc)\n",
    "print(\"Test F1 macro =\", f1m)\n",
    "\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Confusion matrix plot\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(len(class_names))\n",
    "plt.xticks(ticks, class_names, rotation=90)\n",
    "plt.yticks(ticks, class_names)\n",
    "thresh = cm.max()/2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=7)\n",
    "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
    "plt.savefig(\"reports/figures/confusion_matrix.png\", dpi=160)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be0c77",
   "metadata": {},
   "source": [
    "### Application: Predicting New Images\n",
    "\n",
    "To test the trained model on new, unseen images, we define the function `predict_image()`.\n",
    "\n",
    "**How it works**\n",
    "- Loads the final model (`best_finetune.keras` or fallback `best.keras`).\n",
    "- Opens and preprocesses an input image:\n",
    "  - Converts it to RGB.\n",
    "  - Resizes it to **224√ó224** (same as training size).\n",
    "  - Applies the same MobileNetV3 preprocessing (`mobilenet_v3.preprocess_input`).\n",
    "- Runs inference and retrieves the **top-5 predictions** with their probabilities.\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "preds = predict_image(\"playera.jpeg\")\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7cdd6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shirts', 0.961979866027832),\n",
       " ('Tshirts', 0.03419067710638046),\n",
       " ('Tops', 0.003623533295467496),\n",
       " ('Kurtas', 8.044597780099139e-05),\n",
       " ('Casual Shoes', 7.889881817391142e-05)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "CLASS_JSON = \"runs/class_names.json\"\n",
    "with open(CLASS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    CLASS_NAMES = json.load(f)\n",
    "\n",
    "def predict_image(path, model_path=None, topk=5):\n",
    "    model_path = model_path or (\"runs/best_finetune.keras\" if os.path.exists(\"runs/best_finetune.keras\") else \"runs/best.keras\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    img = Image.open(path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    x = np.array(img, dtype=\"float32\")[None, ...]\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(x)\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idxs = probs.argsort()[-topk:][::-1]\n",
    "    return [(CLASS_NAMES[i], float(probs[i])) for i in idxs]\n",
    "\n",
    "# Usage:\n",
    "preds = predict_image(r\"C:\\Users\\sebes\\Trabajo\\7to Semestre\\Bloque 2\\Deep-Learning-Implememtation\\playera.jpeg\")\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba2416",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "This project implemented a CNN-based classifier for fashion product images using transfer learning with MobileNetV3Small. The model achieved strong and consistent performance:\n",
    "\n",
    "- **Test Accuracy:** ~92.3%\n",
    "- **Test F1-macro:** ~0.92\n",
    "\n",
    "The baseline already generalized well thanks to ImageNet pretraining. Performing **fine-tuning on the last 30% of the backbone** with a **low learning rate (1e-5)** further improved validation/test performance by helping the network adapt high-level features (textures, shapes, garment details) to the fashion domain while keeping low-level features stable.\n",
    "\n",
    "**Most confusions** occurred between visually similar categories (e.g., Shirts vs Tshirts, Casual vs Sports Shoes), which is expected given overlapping visual patterns.\n",
    "\n",
    "**Potential improvements:**\n",
    "- Stronger data augmentation (color jitter, random brightness/contrast, Cutout/CutMix).\n",
    "- Higher input resolution (e.g., 256√ó256) and/or stronger backbones (EfficientNetB0/B3).\n",
    "- Class balancing or focal loss if class imbalance increases.\n",
    "- Layer-wise learning rate decay during fine-tuning for more stable updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda88f9",
   "metadata": {},
   "source": [
    "### What changed and why it worked\n",
    "\n",
    "- **Change:** Unfreezing the **top 30%** of MobileNetV3Small and re-training with **LR = 1e-5**.  \n",
    "- **Why it worked:** The top layers capture high-level, task-specific patterns. Allowing them to update (with a small step size) helps specialize from generic ImageNet features to **fashion-specific** cues, improving validation/test metrics without overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f9faa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
